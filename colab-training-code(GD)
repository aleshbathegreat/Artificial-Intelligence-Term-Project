from google.colab import files
files.upload()

import numpy as np
import pandas as pd
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt

dataset = pd.read_csv("mfcc.csv")
print(dataset.shape)

X = dataset.drop(columns=['songid', 'valence_mean', 'arousal_mean']).values

y_val = dataset['valence_mean'].values
y_ars = dataset['arousal_mean'].values

X = np.array(X, dtype = np.float32) 
y_val = np.array(y_val, dtype=np.float32)
y_ars = np.array(y_ars, dtype=np.float32)


def split_data(X, y_val, y_ars, test_ratio=0.2, seed=42):
    total_samples = len(X)
    indices = list(range(total_samples))
    random.seed(seed)
    random.shuffle(indices)

    split = int(total_samples * (1 - test_ratio))
    train_idx, test_idx = indices[:split], indices[split:]

    return (
        X[train_idx], X[test_idx],
        y_val[train_idx], y_val[test_idx],
        y_ars[train_idx], y_ars[test_idx]
    )

class EmotionClassifier(nn.Module):
    def __init__(self, input_size, hidden_size=128):
        super(EmotionClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 2)
        )

    def forward(self, x):
        return self.model(x)

X_train, X_test, y_val_train, y_val_test, y_ars_train, y_ars_test = split_data(X, y_val, y_ars)

# Train
model = EmotionClassifier(input_size=X.shape[1])
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(np.column_stack([y_val_train, y_ars_train]), dtype=torch.float32)
train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)

best_val_loss = float('inf')
epochs_without_improve = 0

losses = []

for epoch in range(35):
    model.train()
    total_loss = 0.0
    for batch_X, batch_y in train_loader:
        pred = model(batch_X)
        loss = criterion(pred, batch_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/100, Loss: {total_loss/len(train_loader):.4f}")
    losses.append(total_loss/len(train_loader))

    #observing drop in training loss to avoid overfitting
    # if (total_loss/len(train_loader)) < best_val_loss:
    #   best_val_loss = total_loss/len(train_loader)
    #   epochs_without_improve = 0
    # else:
    #   epochs_without_improve +=1

    # if epochs_without_improve > 5: #lower tolerance to avoid overfitting
    #   break

model.eval()
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
finalpredictions = model(X_test_tensor).detach().numpy() 

results = pd.DataFrame({
    "val_pred": finalpredictions[:, 0],
    "ars_pred": finalpredictions[:, 1],
    "val_actual": y_val_test,
    "ars_actual": y_ars_test
})

results.to_csv("Predictions_MFCC_GRADIENT.csv", index = False)
torch.save(model.state_dict(), 'emotion_model_weights.pth') #saving model for use

# print("\nSample Inference Results:")
# for i in range(len(predictions)):
#   val_pred, ars_pred = predictions[i]
#   actual_val, actual_ars = y_val_test[i], y_ars_test[i]
#   print(f"Sample {i+1}")
#   print(f"  Predicted: Valence={val_pred:.2f}, Arousal={ars_pred:.2f}")
#   print(f"  Actual   : Valence={actual_val:.2f}, Arousal={actual_ars:.2f}")


#plot during training for analysis
plt.figure(figsize = (5, 4))
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(range(len(losses)), losses, color = 'blue')
plt.legend()
plt.title('Training Loss/Epoch In Gradient Descent')
plt.grid(True)
plt.show()

from google.colab import files
files.download("Predictions_MFCC_GRADIENT.csv")
files.download("emotion_model_weights.pth")
